"""
ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã¨å¯è¦–åŒ–
"""
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import torch


class Evaluator:
    """ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ãƒ»å¯è¦–åŒ–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, model, device):
        """
        Parameters:
        -----------
        model : nn.Module
            è©•ä¾¡ã™ã‚‹ãƒ¢ãƒ‡ãƒ«
        device : torch.device
            ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹
        """
        self.model = model
        self.device = device
    
    def evaluate(self, train_loader, val_loader, test_loader):
        """
        7. è©•ä¾¡
        
        Parameters:
        -----------
        train_loader, val_loader, test_loader : DataLoader
            å„ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        
        Returns:
        --------
        results : dict
            è©•ä¾¡çµæœ
        """
        print("\n" + "=" * 80)
        print("7. ãƒ¢ãƒ‡ãƒ«è©•ä¾¡")
        print("=" * 80)
        
        self.model.eval()
        
        results = {}
        
        for name, loader in [('è¨“ç·´', train_loader), ('æ¤œè¨¼', val_loader), ('ãƒ†ã‚¹ãƒˆ', test_loader)]:
            pred, target, pred_binary = self._evaluate_loader(loader)
            metrics = self._calculate_metrics(target, pred_binary)
            
            results[name] = {
                'predictions': pred,
                'targets': target,
                'predictions_binary': pred_binary,
                'metrics': metrics
            }
            
            self._print_metrics(name, metrics, target, pred_binary)
        
        return results
    
    def _evaluate_loader(self, data_loader):
        """ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã§è©•ä¾¡"""
        all_predictions = []
        all_targets = []
        
        with torch.no_grad():
            for X_batch, y_batch in data_loader:
                X_batch = X_batch.to(self.device)
                outputs = self.model(X_batch)
                all_predictions.extend(outputs.cpu().numpy())
                all_targets.extend(y_batch.numpy())
        
        predictions = np.array(all_predictions)
        targets = np.array(all_targets)
        predictions_binary = (predictions > 0.5).astype(int)
        
        return predictions, targets, predictions_binary
    
    @staticmethod
    def _calculate_metrics(targets, predictions_binary):
        """è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®—"""
        return {
            'accuracy': accuracy_score(targets, predictions_binary),
            'precision': precision_score(targets, predictions_binary, zero_division=0),
            'recall': recall_score(targets, predictions_binary, zero_division=0),
            'f1': f1_score(targets, predictions_binary, zero_division=0),
            'confusion_matrix': confusion_matrix(targets, predictions_binary)
        }
    
    @staticmethod
    def _print_metrics(name, metrics, targets, predictions_binary):
        """è©•ä¾¡çµæœã®è¡¨ç¤º"""
        cm = metrics['confusion_matrix']
        
        if name == 'ãƒ†ã‚¹ãƒˆ':
            # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: åŸºæº–è¶…=1ã®æ­£ç­”ç‡ã‚’è©³ç´°è¡¨ç¤º
            print(f"\n{'='*80}")
            print(f"ã€{name}ãƒ‡ãƒ¼ã‚¿è©•ä¾¡çµæœã€‘")
            print(f"{'='*80}")
            
            actual_1_total = np.sum(targets == 1)
            correct_1 = cm[1][1]
            recall = correct_1 / max(actual_1_total, 1)
            
            print(f"\n  ğŸ¯ åŸºæº–è¶…=1ã®äºˆæ¸¬çµæœ:")
            print(f"    å®Ÿéš›ã«ä¸Šæ˜‡ã—ãŸæ—¥æ•°:     {actual_1_total} æ—¥")
            print(f"    æ­£ã—ãäºˆæ¸¬ã§ããŸæ—¥æ•°:   {correct_1} æ—¥")
            print(f"    æ­£ç­”ç‡ (å†ç¾ç‡):        {recall:.4f} ({recall*100:.2f}%)")
            
            print(f"\n  æ··åŒè¡Œåˆ—:")
            print(f"                äºˆæ¸¬0   äºˆæ¸¬1")
            print(f"    å®Ÿéš›0    {cm[0][0]:6d}  {cm[0][1]:6d}")
            print(f"    å®Ÿéš›1    {cm[1][0]:6d}  {cm[1][1]:6d}")
            
            print(f"\n  å‚è€ƒ:")
            print(f"    é©åˆç‡ (Precision): {metrics['precision']:.4f} (äºˆæ¸¬ãŒå½“ãŸã‚‹ç¢ºç‡)")
            print(f"    å…¨ä½“ã®æ­£è§£ç‡:       {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)")
            
        else:
            # è¨“ç·´ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿: ç°¡æ˜“è¡¨ç¤ºï¼ˆéå­¦ç¿’ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰
            recall = metrics['recall']
            print(f"\nã€{name}ãƒ‡ãƒ¼ã‚¿ã€‘å…¨ä½“æ­£è§£ç‡: {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%) | åŸºæº–è¶…=1ã®æ­£ç­”ç‡: {recall:.4f} ({recall*100:.2f}%)")
    
    def visualize(self, results, train_losses, val_losses, epochs):
        """
        8. çµæœã®å¯è¦–åŒ–
        
        Parameters:
        -----------
        results : dict
            è©•ä¾¡çµæœ
        train_losses, val_losses : list
            è¨“ç·´ãƒ»æ¤œè¨¼ã®Losså±¥æ­´
        epochs : int
            ã‚¨ãƒãƒƒã‚¯æ•°
        """
        print("\n" + "=" * 80)
        print("8. çµæœã®å¯è¦–åŒ–")
        print("=" * 80)
        
        plt.rcParams['font.sans-serif'] = ['MS Gothic', 'Yu Gothic', 'Hiragino Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # ã‚°ãƒ©ãƒ•1: Lossã®æ¸›è¡°
        ax1.plot(range(1, epochs+1), train_losses, label='è¨“ç·´Loss', marker='o', linewidth=2)
        ax1.plot(range(1, epochs+1), val_losses, label='æ¤œè¨¼Loss', marker='s', linewidth=2)
        ax1.set_xlabel('Epoch', fontsize=12)
        ax1.set_ylabel('Loss', fontsize=12)
        ax1.set_title('Lossã®æ¸›è¡°', fontsize=14, fontweight='bold')
        ax1.legend(fontsize=11)
        ax1.grid(True, alpha=0.3)
        
        # ã‚°ãƒ©ãƒ•2: ãƒ¢ãƒ‡ãƒ«ãŒ1ã¨äºˆæ¸¬ã—ãŸã¨ãã®çš„ä¸­ç‡
        datasets = ['è¨“ç·´', 'æ¤œè¨¼', 'ãƒ†ã‚¹ãƒˆ']
        precisions = []
        predicted_counts = []
        correct_counts = []
        
        for name in datasets:
            predictions = results[name]['predictions']
            targets = results[name]['targets']
            
            # äºˆæ¸¬ã‚¯ãƒ©ã‚¹ï¼ˆé–¾å€¤0.5ï¼‰
            pred_binary = (predictions > 0.5).astype(int)
            
            # ãƒ¢ãƒ‡ãƒ«ãŒ1ã¨äºˆæ¸¬ã—ãŸã‚±ãƒ¼ã‚¹
            predicted_positive = pred_binary == 1
            n_predicted_positive = np.sum(predicted_positive)
            
            # ãã®ä¸­ã§å®Ÿéš›ã«1ã ã£ãŸã‚±ãƒ¼ã‚¹
            true_positive = np.sum((pred_binary == 1) & (targets == 1))
            
            # çš„ä¸­ç‡ï¼ˆé©åˆç‡ï¼‰
            precision = true_positive / n_predicted_positive if n_predicted_positive > 0 else 0
            
            precisions.append(precision)
            predicted_counts.append(n_predicted_positive)
            correct_counts.append(true_positive)
        
        bars = ax2.bar(datasets, precisions, color=['skyblue', 'lightgreen', 'lightcoral'])
        
        ax2.set_ylabel('çš„ä¸­ç‡', fontsize=12)
        ax2.set_title('ãƒ¢ãƒ‡ãƒ«ãŒã€Œ1ã€ã¨äºˆæ¸¬ã—ãŸã¨ãã®çš„ä¸­ç‡', fontsize=14, fontweight='bold')
        ax2.set_ylim(0, 1.0)
        ax2.grid(True, alpha=0.3, axis='y')
        
        # æ•°å€¤ãƒ©ãƒ™ãƒ«
        for i, (bar, precision) in enumerate(zip(bars, precisions)):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                    f'{precision*100:.1f}%',
                    ha='center', va='bottom', fontsize=13, fontweight='bold')
            
            # äºˆæ¸¬æ•°ã‚’æ£’ã®ä¸‹ã«è¡¨ç¤º
            ax2.text(bar.get_x() + bar.get_width()/2., -0.05,
                    f'{predicted_counts[i]}ä»¶ä¸­\n{correct_counts[i]}ä»¶çš„ä¸­',
                    ha='center', va='top', fontsize=10)
        
        plt.tight_layout()
        plt.show()