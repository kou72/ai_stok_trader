"""
ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã¨å¯è¦–åŒ–
"""
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
import torch


class Evaluator:
    """ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ãƒ»å¯è¦–åŒ–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, model, device):
        """
        Parameters:
        -----------
        model : nn.Module
            è©•ä¾¡ã™ã‚‹ãƒ¢ãƒ‡ãƒ«
        device : torch.device
            ä½¿ç”¨ãƒ‡ãƒã‚¤ã‚¹
        """
        self.model = model
        self.device = device
    
    def evaluate(self, train_loader, val_loader, test_loader):
        """
        7. è©•ä¾¡
        
        Parameters:
        -----------
        train_loader, val_loader, test_loader : DataLoader
            å„ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼
        
        Returns:
        --------
        results : dict
            è©•ä¾¡çµæœ
        """
        print("\n" + "=" * 80)
        print("7. ãƒ¢ãƒ‡ãƒ«è©•ä¾¡")
        print("=" * 80)
        
        self.model.eval()
        
        results = {}
        
        for name, loader in [('è¨“ç·´', train_loader), ('æ¤œè¨¼', val_loader), ('ãƒ†ã‚¹ãƒˆ', test_loader)]:
            pred, target, pred_binary = self._evaluate_loader(loader)
            metrics = self._calculate_metrics(target, pred_binary)
            
            results[name] = {
                'predictions': pred,
                'targets': target,
                'predictions_binary': pred_binary,
                'metrics': metrics
            }
            
            self._print_metrics(name, metrics, target, pred_binary)
        
        return results
    
    def _evaluate_loader(self, data_loader):
        """ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã§è©•ä¾¡"""
        all_predictions = []
        all_targets = []
        
        with torch.no_grad():
            for X_batch, y_batch in data_loader:
                X_batch = X_batch.to(self.device)
                outputs = self.model(X_batch)
                all_predictions.extend(outputs.cpu().numpy())
                all_targets.extend(y_batch.numpy())
        
        predictions = np.array(all_predictions)
        targets = np.array(all_targets)
        predictions_binary = (predictions > 0.5).astype(int)
        
        return predictions, targets, predictions_binary
    
    @staticmethod
    def _calculate_metrics(targets, predictions_binary):
        """è©•ä¾¡æŒ‡æ¨™ã®è¨ˆç®—"""
        return {
            'accuracy': accuracy_score(targets, predictions_binary),
            'precision': precision_score(targets, predictions_binary, zero_division=0),
            'recall': recall_score(targets, predictions_binary, zero_division=0),
            'f1': f1_score(targets, predictions_binary, zero_division=0),
            'confusion_matrix': confusion_matrix(targets, predictions_binary)
        }
    
    @staticmethod
    def _print_metrics(name, metrics, targets, predictions_binary):
        """è©•ä¾¡çµæœã®è¡¨ç¤º"""
        cm = metrics['confusion_matrix']
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å ´åˆã¯åŸºæº–è¶…=1ã®æ­£ç­”ç‡ã®ã¿è¡¨ç¤º
        if name == 'ãƒ†ã‚¹ãƒˆ':
            print(f"\n{'='*80}")
            print(f"ã€{name}ãƒ‡ãƒ¼ã‚¿è©•ä¾¡çµæœã€‘")
            print(f"{'='*80}")
            
            actual_1_total = np.sum(targets == 1)  # å®Ÿéš›ã«åŸºæº–è¶…=1ã ã£ãŸæ•°
            correct_1 = cm[1][1]  # æ­£ã—ãäºˆæ¸¬ã§ããŸæ•°
            recall = correct_1 / max(actual_1_total, 1)
            
            print(f"\n  ğŸ¯ åŸºæº–è¶…=1ã®äºˆæ¸¬çµæœ:")
            print(f"    å®Ÿéš›ã«ä¸Šæ˜‡ã—ãŸæ—¥æ•°:     {actual_1_total} æ—¥")
            print(f"    æ­£ã—ãäºˆæ¸¬ã§ããŸæ—¥æ•°:   {correct_1} æ—¥")
            print(f"    æ­£ç­”ç‡:                 {recall:.4f} ({recall*100:.2f}%)")
            
            print(f"\n  å‚è€ƒ:")
            print(f"    å…¨ä½“ã®æ­£è§£ç‡: {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)")
            
        else:
            # è¨“ç·´ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã¯è©³ç´°ã‚’è¡¨ç¤º
            print(f"\n{'='*80}")
            print(f"ã€{name}ãƒ‡ãƒ¼ã‚¿è©•ä¾¡çµæœã€‘")
            print(f"{'='*80}")
            print(f"  æ­£è§£ç‡ (Accuracy):  {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)")
            print(f"  é©åˆç‡ (Precision): {metrics['precision']:.4f}")
            print(f"  å†ç¾ç‡ (Recall):    {metrics['recall']:.4f}")
            print(f"  F1ã‚¹ã‚³ã‚¢:           {metrics['f1']:.4f}")
            print(f"\n  æ··åŒè¡Œåˆ—:")
            print(f"                äºˆæ¸¬0   äºˆæ¸¬1")
            print(f"    å®Ÿéš›0    {cm[0][0]:6d}  {cm[0][1]:6d}")
            print(f"    å®Ÿéš›1    {cm[1][0]:6d}  {cm[1][1]:6d}")
            
            # ã‚ã‹ã‚Šã‚„ã™ã„æ­£ç­”ç‡ã®è¡¨ç¤º
            total = len(targets)
            correct_0 = cm[0][0]
            correct_1 = cm[1][1]
            
            print(f"\n  è©³ç´°:")
            print(f"    åŸºæº–è¶…=0ã®æ­£è§£æ•°: {correct_0}/{np.sum(targets==0)} ({correct_0/max(np.sum(targets==0),1)*100:.2f}%)")
            print(f"    åŸºæº–è¶…=1ã®æ­£è§£æ•°: {correct_1}/{np.sum(targets==1)} ({correct_1/max(np.sum(targets==1),1)*100:.2f}%)")
            print(f"    å…¨ä½“ã®æ­£è§£æ•°:     {correct_0+correct_1}/{total} ({(correct_0+correct_1)/total*100:.2f}%)")
    
    def visualize(self, results, train_losses, val_losses, epochs):
        """
        8. çµæœã®å¯è¦–åŒ–
        
        Parameters:
        -----------
        results : dict
            è©•ä¾¡çµæœ
        train_losses, val_losses : list
            è¨“ç·´ãƒ»æ¤œè¨¼ã®Losså±¥æ­´
        epochs : int
            ã‚¨ãƒãƒƒã‚¯æ•°
        """
        print("\n" + "=" * 80)
        print("8. çµæœã®å¯è¦–åŒ–")
        print("=" * 80)
        
        # ã‚°ãƒ©ãƒ•1: å­¦ç¿’æ›²ç·š
        self._plot_learning_curve(train_losses, val_losses, epochs)
        
        # ã‚°ãƒ©ãƒ•2: æ··åŒè¡Œåˆ—
        self._plot_confusion_matrices(results)
        
        # ã‚°ãƒ©ãƒ•3: äºˆæ¸¬ç¢ºç‡ã®åˆ†å¸ƒ
        self._plot_prediction_distributions(results)
        
        # ã‚°ãƒ©ãƒ•4: æ­£ç­”ç‡ã®ã‚µãƒãƒªãƒ¼
        self._plot_accuracy_summary(results)
    
    @staticmethod
    def _plot_learning_curve(train_losses, val_losses, epochs):
        """å­¦ç¿’æ›²ç·š"""
        plt.rcParams['font.sans-serif'] = ['MS Gothic', 'Yu Gothic', 'Hiragino Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.plot(range(1, epochs+1), train_losses, label='è¨“ç·´Loss', marker='o')
        ax.plot(range(1, epochs+1), val_losses, label='æ¤œè¨¼Loss', marker='s')
        ax.set_xlabel('Epoch')
        ax.set_ylabel('Loss')
        ax.set_title('å­¦ç¿’æ›²ç·šï¼ˆLossï¼‰', fontsize=14, fontweight='bold')
        ax.legend()
        ax.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.show()
    
    @staticmethod
    def _plot_confusion_matrices(results):
        """æ··åŒè¡Œåˆ—"""
        plt.rcParams['font.sans-serif'] = ['MS Gothic', 'Yu Gothic', 'Hiragino Sans']
        
        fig, axes = plt.subplots(1, 3, figsize=(15, 4))
        fig.suptitle('æ··åŒè¡Œåˆ—', fontsize=16, fontweight='bold')
        
        for idx, name in enumerate(['è¨“ç·´', 'æ¤œè¨¼', 'ãƒ†ã‚¹ãƒˆ']):
            cm = results[name]['metrics']['confusion_matrix']
            im = axes[idx].imshow(cm, cmap='Blues')
            axes[idx].set_title(f'{name}ãƒ‡ãƒ¼ã‚¿')
            axes[idx].set_xlabel('äºˆæ¸¬')
            axes[idx].set_ylabel('å®Ÿéš›')
            axes[idx].set_xticks([0, 1])
            axes[idx].set_yticks([0, 1])
            axes[idx].set_xticklabels(['åŸºæº–è¶…=0', 'åŸºæº–è¶…=1'])
            axes[idx].set_yticklabels(['åŸºæº–è¶…=0', 'åŸºæº–è¶…=1'])
            
            for i in range(2):
                for j in range(2):
                    axes[idx].text(j, i, str(cm[i, j]),
                                  ha='center', va='center',
                                  color='white' if cm[i, j] > cm.max()/2 else 'black',
                                  fontsize=14, fontweight='bold')
            
            plt.colorbar(im, ax=axes[idx])
        
        plt.tight_layout()
        plt.show()
    
    @staticmethod
    def _plot_prediction_distributions(results):
        """äºˆæ¸¬ç¢ºç‡ã®åˆ†å¸ƒ"""
        fig, axes = plt.subplots(1, 3, figsize=(15, 4))
        fig.suptitle('äºˆæ¸¬ç¢ºç‡ã®åˆ†å¸ƒ', fontsize=16, fontweight='bold')
        
        for idx, name in enumerate(['è¨“ç·´', 'æ¤œè¨¼', 'ãƒ†ã‚¹ãƒˆ']):
            pred = results[name]['predictions']
            target = results[name]['targets']
            
            axes[idx].hist(pred[target==0], bins=30, alpha=0.6, label='å®Ÿéš›0', color='blue')
            axes[idx].hist(pred[target==1], bins=30, alpha=0.6, label='å®Ÿéš›1', color='red')
            axes[idx].axvline(x=0.5, color='green', linestyle='--', label='é–¾å€¤0.5')
            axes[idx].set_title(f'{name}ãƒ‡ãƒ¼ã‚¿')
            axes[idx].set_xlabel('äºˆæ¸¬ç¢ºç‡')
            axes[idx].set_ylabel('é »åº¦')
            axes[idx].legend()
            axes[idx].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    @staticmethod
    def _plot_accuracy_summary(results):
        """æ­£ç­”ç‡ã®ã‚µãƒãƒªãƒ¼"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # ã‚°ãƒ©ãƒ•1: å„æŒ‡æ¨™ã®æ¯”è¼ƒ
        datasets = ['è¨“ç·´', 'æ¤œè¨¼', 'ãƒ†ã‚¹ãƒˆ']
        metrics_names = ['æ­£è§£ç‡', 'é©åˆç‡', 'å†ç¾ç‡', 'F1']
        
        x = np.arange(len(datasets))
        width = 0.2
        
        for i, metric_key in enumerate(['accuracy', 'precision', 'recall', 'f1']):
            values = [results[name]['metrics'][metric_key] for name in datasets]
            ax1.bar(x + i*width, values, width, label=metrics_names[i])
        
        ax1.set_xlabel('ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ', fontsize=12)
        ax1.set_ylabel('ã‚¹ã‚³ã‚¢', fontsize=12)
        ax1.set_title('è©•ä¾¡æŒ‡æ¨™ã®æ¯”è¼ƒ', fontsize=14, fontweight='bold')
        ax1.set_xticks(x + width*1.5)
        ax1.set_xticklabels(datasets)
        ax1.legend()
        ax1.grid(True, alpha=0.3, axis='y')
        ax1.set_ylim(0, 1.0)
        
        # æ•°å€¤ãƒ©ãƒ™ãƒ«
        for container in ax1.containers:
            ax1.bar_label(container, fmt='%.3f', fontsize=8)
        
        # ã‚°ãƒ©ãƒ•2: æ­£ç­”ç‡ã®å†…è¨³
        for idx, name in enumerate(datasets):
            cm = results[name]['metrics']['confusion_matrix']
            total = cm.sum()
            correct_0 = cm[0][0]
            correct_1 = cm[1][1]
            incorrect = cm[0][1] + cm[1][0]
            
            ax2.bar(idx, correct_0, label='åŸºæº–è¶…=0 æ­£è§£' if idx==0 else '', color='lightblue')
            ax2.bar(idx, correct_1, bottom=correct_0, label='åŸºæº–è¶…=1 æ­£è§£' if idx==0 else '', color='lightcoral')
            ax2.bar(idx, incorrect, bottom=correct_0+correct_1, label='ä¸æ­£è§£' if idx==0 else '', color='lightgray')
            
            # æ­£è§£ç‡ã‚’ä¸Šéƒ¨ã«è¡¨ç¤º
            accuracy = (correct_0 + correct_1) / total
            ax2.text(idx, total + 5, f'{accuracy*100:.2f}%', ha='center', fontsize=12, fontweight='bold')
        
        ax2.set_xlabel('ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ', fontsize=12)
        ax2.set_ylabel('ã‚µãƒ³ãƒ—ãƒ«æ•°', fontsize=12)
        ax2.set_title('æ­£ç­”ç‡ã®å†…è¨³', fontsize=14, fontweight='bold')
        ax2.set_xticks(range(len(datasets)))
        ax2.set_xticklabels(datasets)
        ax2.legend()
        ax2.grid(True, alpha=0.3, axis='y')
        
        plt.tight_layout()
        plt.show()